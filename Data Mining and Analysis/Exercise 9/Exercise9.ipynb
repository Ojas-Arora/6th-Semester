{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "MMNP5dWhAWVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_rows = []\n",
        "invalid_rows = []\n",
        "\n",
        "with open(\"basket.csv\", \"r\") as f:\n",
        "    for i, line in enumerate(f, start=1):\n",
        "        fields = line.strip().split(\",\")\n",
        "        if len(fields) == 5:\n",
        "            cleaned_rows.append(line.strip())\n",
        "        else:\n",
        "            invalid_rows.append((i, line.strip()))\n",
        "with open(\"basket_cleaned.csv\", \"w\") as f:\n",
        "    for row in cleaned_rows:\n",
        "        f.write(row + \"\\n\")\n",
        "\n",
        "print(f\"Cleaning complete: {len(cleaned_rows)} valid rows saved.\")\n",
        "print(f\"{len(invalid_rows)} invalid rows removed.\")\n"
      ],
      "metadata": {
        "id": "QF1g9_5BsgU8",
        "outputId": "589c12bd-ad04-4901-cd23-1e9c6946c980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning complete: 1104 valid rows saved.\n",
            "316 invalid rows removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function is used to generate all the subset of any set"
      ],
      "metadata": {
        "id": "Zp1_Rp1MBXZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dksTjH7KOv42"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "def genSubsets(l):\n",
        "    powerSetSize = 2 ** len(l)\n",
        "    powerSet = []\n",
        "    for i in range(1, powerSetSize):\n",
        "        tempEle = []\n",
        "        for j in range(len(l)):\n",
        "            if i & (1 << j):\n",
        "                tempEle.append(l[j])\n",
        "        powerSet.append(tempEle)\n",
        "    return powerSet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initPass(txList):\n",
        "    allTx = [item for tx in txList for item in tx]\n",
        "    allTx.sort()\n",
        "    cntr = OrderedDict()\n",
        "    for tx in allTx:\n",
        "        cntr[tx] = cntr.get(tx, 0) + 1\n",
        "    return cntr\n"
      ],
      "metadata": {
        "id": "c4NyKqJ0CNNy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apriori Algorithm Implementation"
      ],
      "metadata": {
        "id": "NrgRel7NCYvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def genCandidate(Fk1):\n",
        "    Ck = []\n",
        "    k1 = len(Fk1[0])\n",
        "    for i in range(len(Fk1) - 1):\n",
        "        for j in range(i + 1, len(Fk1)):\n",
        "            f1, f2 = Fk1[i], Fk1[j]\n",
        "            if f1[:len(f1) - 1] == f2[:len(f2) - 1] and f1[-1] < f2[-1]:\n",
        "                tempC = f1 + [f2[-1]]\n",
        "                subset = genSubsets(tempC)\n",
        "                appendSts = True\n",
        "                for item in subset:\n",
        "                    if len(item) == k1 and item not in Fk1:\n",
        "                        appendSts = False\n",
        "                if appendSts:\n",
        "                    Ck.append(tempC)\n",
        "    return Ck"
      ],
      "metadata": {
        "id": "GlLCnkRhCUXh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def searchInT(t, candid):\n",
        "    return all(eachCandid in t for eachCandid in candid)\n",
        "\n",
        "def apriori(T, minSup):\n",
        "    finalSet = []\n",
        "    c1 = initPass(T)\n",
        "    f = [[item] for item in c1.keys() if c1[item] / len(T) >= minSup]\n",
        "    for item in f:\n",
        "        finalSet.append(item)\n",
        "    while len(f) != 0:\n",
        "        Ck = genCandidate(f)\n",
        "        freqDict = {}\n",
        "        for t in T:\n",
        "            for candidate in Ck:\n",
        "                if searchInT(t, candidate):\n",
        "                    freqDict[tuple(candidate)] = freqDict.get(tuple(candidate), 0) + 1\n",
        "        f = []\n",
        "        for c in freqDict.keys():\n",
        "            if freqDict[c] / len(T) >= minSup:\n",
        "                f.append(list(c))\n",
        "        if len(f) != 0:\n",
        "            f = sorted(f, key=lambda x: (len(x), *x))\n",
        "            for item in f:\n",
        "                finalSet.append(item)\n",
        "    return finalSet\n",
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    rules = []\n",
        "    total_tx = len(transactions)\n",
        "    itemset_counts = {}\n",
        "    for itemset in frequent_itemsets:\n",
        "        count = sum(1 for tx in transactions if set(itemset).issubset(set(tx)))\n",
        "        itemset_counts[tuple(itemset)] = count / total_tx\n",
        "    for itemset in frequent_itemsets:\n",
        "        if len(itemset) < 2:\n",
        "            continue\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = list(antecedent)\n",
        "                consequent = [x for x in itemset if x not in antecedent]\n",
        "                antecedent_support = itemset_counts.get(tuple(antecedent), 0)\n",
        "                rule_support = itemset_counts[tuple(itemset)]\n",
        "                if antecedent_support > 0:\n",
        "                    confidence = rule_support / antecedent_support\n",
        "                    if confidence >= min_confidence:\n",
        "                        rules.append((antecedent, consequent, rule_support, confidence))\n",
        "    return rules"
      ],
      "metadata": {
        "id": "XnfnMbhOCesd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running on Different Support and confidence values"
      ],
      "metadata": {
        "id": "e9PE1MVmCpA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"basket_cleaned.csv\", header=None, names=['Cert1', 'Cert2', 'Ethnicity', 'City', 'ZIP'])\n",
        "transactions = []\n",
        "for _, row in data.iterrows():\n",
        "    items = []\n",
        "    if pd.notna(row['Cert1']): items.append(row['Cert1'])\n",
        "    if pd.notna(row['Cert2']): items.append(row['Cert2'])\n",
        "    if pd.notna(row['Ethnicity']): items.append(row['Ethnicity'])\n",
        "    transactions.append(items)\n",
        "\n",
        "support_values = [0.05, 0.1, 0.2]\n",
        "confidence_values = [0.1,0.3,0.5, 0.7, 0.9]\n",
        "\n",
        "for min_support in support_values:\n",
        "    for min_confidence in confidence_values:\n",
        "        print(f\"\\nSupport: {min_support}, Confidence: {min_confidence}\")\n",
        "        frequent_itemsets = apriori(transactions, min_support)\n",
        "        rules = generate_rules(frequent_itemsets, transactions, min_confidence)\n",
        "\n",
        "        print(f\"Number of frequent itemsets: {len(frequent_itemsets)}\")\n",
        "        print(f\"Number of rules: {len(rules)}\")\n",
        "        for rule in rules[:5]:\n",
        "            antecedent, consequent, support, confidence = rule\n",
        "            print(f\"{antecedent} -> {consequent}, Support: {support:.3f}, Confidence: {confidence:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_mbi2Fhymy-",
        "outputId": "c6261426-0ee4-475f-b636-a64511d8cf4d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Support: 0.05, Confidence: 0.1\n",
            "Number of frequent itemsets: 23\n",
            "Number of rules: 38\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['MBE'] -> ['ASIAN'], Support: 0.196, Confidence: 0.319\n",
            "['ASIAN'] -> ['New York'], Support: 0.056, Confidence: 0.283\n",
            "['New York'] -> ['ASIAN'], Support: 0.056, Confidence: 0.184\n",
            "['BLACK'] -> ['Brooklyn'], Support: 0.060, Confidence: 0.224\n",
            "\n",
            "Support: 0.05, Confidence: 0.3\n",
            "Number of frequent itemsets: 23\n",
            "Number of rules: 26\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['MBE'] -> ['ASIAN'], Support: 0.196, Confidence: 0.319\n",
            "['Brooklyn'] -> ['BLACK'], Support: 0.060, Confidence: 0.415\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['MBE'] -> ['BLACK'], Support: 0.267, Confidence: 0.436\n",
            "\n",
            "Support: 0.05, Confidence: 0.5\n",
            "Number of frequent itemsets: 23\n",
            "Number of rules: 13\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['Bronx'] -> ['MBE'], Support: 0.063, Confidence: 0.909\n",
            "['Brooklyn'] -> ['MBE'], Support: 0.100, Confidence: 0.692\n",
            "['HISPANIC'] -> ['MBE'], Support: 0.146, Confidence: 1.000\n",
            "\n",
            "Support: 0.05, Confidence: 0.7\n",
            "Number of frequent itemsets: 23\n",
            "Number of rules: 10\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['Bronx'] -> ['MBE'], Support: 0.063, Confidence: 0.909\n",
            "['HISPANIC'] -> ['MBE'], Support: 0.146, Confidence: 1.000\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "\n",
            "Support: 0.05, Confidence: 0.9\n",
            "Number of frequent itemsets: 23\n",
            "Number of rules: 10\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['Bronx'] -> ['MBE'], Support: 0.063, Confidence: 0.909\n",
            "['HISPANIC'] -> ['MBE'], Support: 0.146, Confidence: 1.000\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "\n",
            "Support: 0.1, Confidence: 0.1\n",
            "Number of frequent itemsets: 16\n",
            "Number of rules: 20\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['MBE'] -> ['ASIAN'], Support: 0.196, Confidence: 0.319\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['MBE'] -> ['BLACK'], Support: 0.267, Confidence: 0.436\n",
            "['HISPANIC'] -> ['MBE'], Support: 0.146, Confidence: 1.000\n",
            "\n",
            "Support: 0.1, Confidence: 0.3\n",
            "Number of frequent itemsets: 16\n",
            "Number of rules: 18\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['MBE'] -> ['ASIAN'], Support: 0.196, Confidence: 0.319\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['MBE'] -> ['BLACK'], Support: 0.267, Confidence: 0.436\n",
            "['HISPANIC'] -> ['MBE'], Support: 0.146, Confidence: 1.000\n",
            "\n",
            "Support: 0.1, Confidence: 0.5\n",
            "Number of frequent itemsets: 16\n",
            "Number of rules: 8\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['HISPANIC'] -> ['MBE'], Support: 0.146, Confidence: 1.000\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "['WBE'] -> ['NON-MINORITY'], Support: 0.382, Confidence: 0.981\n",
            "\n",
            "Support: 0.1, Confidence: 0.7\n",
            "Number of frequent itemsets: 16\n",
            "Number of rules: 7\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['HISPANIC'] -> ['MBE'], Support: 0.146, Confidence: 1.000\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "['WBE'] -> ['NON-MINORITY'], Support: 0.382, Confidence: 0.981\n",
            "\n",
            "Support: 0.1, Confidence: 0.9\n",
            "Number of frequent itemsets: 16\n",
            "Number of rules: 7\n",
            "['ASIAN'] -> ['MBE'], Support: 0.196, Confidence: 0.986\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['HISPANIC'] -> ['MBE'], Support: 0.146, Confidence: 1.000\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "['WBE'] -> ['NON-MINORITY'], Support: 0.382, Confidence: 0.981\n",
            "\n",
            "Support: 0.2, Confidence: 0.1\n",
            "Number of frequent itemsets: 7\n",
            "Number of rules: 4\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['MBE'] -> ['BLACK'], Support: 0.267, Confidence: 0.436\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "['WBE'] -> ['NON-MINORITY'], Support: 0.382, Confidence: 0.981\n",
            "\n",
            "Support: 0.2, Confidence: 0.3\n",
            "Number of frequent itemsets: 7\n",
            "Number of rules: 4\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['MBE'] -> ['BLACK'], Support: 0.267, Confidence: 0.436\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "['WBE'] -> ['NON-MINORITY'], Support: 0.382, Confidence: 0.981\n",
            "\n",
            "Support: 0.2, Confidence: 0.5\n",
            "Number of frequent itemsets: 7\n",
            "Number of rules: 3\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "['WBE'] -> ['NON-MINORITY'], Support: 0.382, Confidence: 0.981\n",
            "\n",
            "Support: 0.2, Confidence: 0.7\n",
            "Number of frequent itemsets: 7\n",
            "Number of rules: 3\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "['WBE'] -> ['NON-MINORITY'], Support: 0.382, Confidence: 0.981\n",
            "\n",
            "Support: 0.2, Confidence: 0.9\n",
            "Number of frequent itemsets: 7\n",
            "Number of rules: 3\n",
            "['BLACK'] -> ['MBE'], Support: 0.267, Confidence: 1.000\n",
            "['NON-MINORITY'] -> ['WBE'], Support: 0.382, Confidence: 1.000\n",
            "['WBE'] -> ['NON-MINORITY'], Support: 0.382, Confidence: 0.981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum size of rule that can be created"
      ],
      "metadata": {
        "id": "Po0hANp0EuuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = []\n",
        "for _, row in data.iterrows():\n",
        "    items = []\n",
        "    if pd.notna(row['Cert1']): items.append(row['Cert1'])\n",
        "    if pd.notna(row['Cert2']): items.append(row['Cert2'])\n",
        "    if pd.notna(row['Ethnicity']): items.append(row['Ethnicity'])\n",
        "    transactions.append(items)\n",
        "\n",
        "min_support = 0.05\n",
        "frequent_itemsets = apriori(transactions, min_support)\n",
        "max_size = max(len(itemset) for itemset in frequent_itemsets) if frequent_itemsets else 0\n",
        "print(f\"Maximum size of frequent itemsets: {max_size}\")\n",
        "print(f\"Maximum size of rule possible: {max_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfkg2Px4DPhm",
        "outputId": "a933bb95-7efe-4efc-8ee7-f39ace9331d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum size of frequent itemsets: 3\n",
            "Maximum size of rule possible: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = []\n",
        "for _, row in data.iterrows():\n",
        "    items = []\n",
        "    if pd.notna(row['Cert1']): items.append(row['Cert1'])\n",
        "    if pd.notna(row['Cert2']): items.append(row['Cert2'])\n",
        "    if pd.notna(row['Ethnicity']): items.append(row['Ethnicity'])\n",
        "    transactions.append(items)\n",
        "\n",
        "min_support = 0.1\n",
        "frequent_itemsets = apriori(transactions, min_support)\n",
        "max_size = max(len(itemset) for itemset in frequent_itemsets) if frequent_itemsets else 0\n",
        "print(f\"Maximum size of frequent itemsets: {max_size}\")\n",
        "print(f\"Maximum size of rule possible: {max_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txjiXAcrFE7u",
        "outputId": "0e318b12-3645-4ab7-f5cf-85f15b7c95ed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum size of frequent itemsets: 3\n",
            "Maximum size of rule possible: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = []\n",
        "for _, row in data.iterrows():\n",
        "    items = []\n",
        "    if pd.notna(row['Cert1']): items.append(row['Cert1'])\n",
        "    if pd.notna(row['Cert2']): items.append(row['Cert2'])\n",
        "    if pd.notna(row['Ethnicity']): items.append(row['Ethnicity'])\n",
        "    transactions.append(items)\n",
        "\n",
        "min_support = 0.2\n",
        "frequent_itemsets = apriori(transactions, min_support)\n",
        "max_size = max(len(itemset) for itemset in frequent_itemsets) if frequent_itemsets else 0\n",
        "print(f\"Maximum size of frequent itemsets: {max_size}\")\n",
        "print(f\"Maximum size of rule possible: {max_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRfP3RUGFK8a",
        "outputId": "2685e3b0-0382-4be5-961b-039aaf250006"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum size of frequent itemsets: 2\n",
            "Maximum size of rule possible: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Confidence value for which Minimum number of rules are generated"
      ],
      "metadata": {
        "id": "Z-1Daza1FXK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_support = 0.05\n",
        "confidence_values = [0.1,0.3,0.5, 0.7, 0.9]\n",
        "frequent_itemsets = apriori(transactions, min_support)\n",
        "\n",
        "rule_counts = {}\n",
        "for min_confidence in confidence_values:\n",
        "    rules = generate_rules(frequent_itemsets, transactions, min_confidence)\n",
        "    rule_counts[min_confidence] = len(rules)\n",
        "    print(f\"Confidence: {min_confidence}, Number of rules: {len(rules)}\")\n",
        "\n",
        "min_rules_conf = min(rule_counts, key=rule_counts.get)\n",
        "print(f\"\\nConfidence value with minimum number of rules: {min_rules_conf} ({rule_counts[min_rules_conf]} rules)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heu3hHBUFogu",
        "outputId": "f281d1d3-94ea-48d5-e520-106757993039"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confidence: 0.1, Number of rules: 38\n",
            "Confidence: 0.3, Number of rules: 26\n",
            "Confidence: 0.5, Number of rules: 13\n",
            "Confidence: 0.7, Number of rules: 10\n",
            "Confidence: 0.9, Number of rules: 10\n",
            "\n",
            "Confidence value with minimum number of rules: 0.7 (10 rules)\n"
          ]
        }
      ]
    }
  ]
}